{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 Análisis de Opiniones sobre Películas\n",
    "\n",
    "El análisis de sentimiento (o minería de opiniones) se refiere al proceso de extraer información acerca de la actitud que una persona (o grupo de ellas) manifiesta, en un determinado medio o formato digital, con respecto a un tópico o contexto de comunicación.\n",
    "\n",
    "Los datos que se usarán corresponden a un subconjunto de los datos publicados en Kaggle, en el contexto de una competencia organizada por la Universidad de Stanford. Cada registro disponible correspondería a una opinión sobre una película, registrada sobre el sitio Rotten Tomatoes. Para empezar se limitará a estudiar textos anotados como positivos o negativos, clases que se codificarán como +1 y 0 respectivamente. Para construir un clasificador que determine automáticamente la polaridad de un trozo de texto, se va a necesitar representar los textos disponibles como vectores de características (features). El tipo de características más utilizado consiste en contar cuántas veces aparecen ciertos términos/palabras en el texto. Para esto, se necesita un vocabulario que, para esta actividad, se construirá mediante la unión de todas las palabras que observemos en los textos que se tienen a disposición. Para aumentar la eficacia de las características extraídas es conveniente ejecutar algunas técnicas de pre-procesamiento básicas como: pasar todo el texto a minúsculas (lower-casing), eliminar signos de puntuación y eliminar palabras sin significado como artículos, pronombres y preposiciones (stop word removal). Otra técnica que suele ser útil para obtener buenas características (features) es la lematización, es decir la reducción de todas las palabras a su tronco léxico base. Una técnica similar y más utilizada en la práctica es el stemming.\n",
    "\n",
    "\n",
    "### 2.2.a. Construcción de dataframe con los datos a analizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentimiento negativo en los datos de entrenamiento: 1803\n",
      "Sentimiento positivo en los datos de entrenamiento: 1751\n",
      "Sentimiento negativo en los datos de prueba: 1803\n",
      "Sentimiento positivo en los datos de prueba: 1751\n",
      "(3554, 2)\n",
      "(3554, 2)\n"
     ]
    }
   ],
   "source": [
    "#2.a\n",
    "import urllib\n",
    "import pandas as pd\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "positivo_train=0\n",
    "negativo_train=0\n",
    "for x in train_df['Sentiment']:\n",
    "    if x==-1:\n",
    "        negativo_train+=1\n",
    "    if x==+1:\n",
    "        positivo_train+=1\n",
    "print \"Sentimiento negativo en los datos de entrenamiento: %d\"%negativo_train\n",
    "print \"Sentimiento positivo en los datos de entrenamiento: %d\"%positivo_train\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])\n",
    "positivo_test=0\n",
    "negativo_test=0\n",
    "for x in test_df['Sentiment']:\n",
    "    if x==-1:\n",
    "        negativo_test+=1\n",
    "    if x==1:\n",
    "        positivo_test+=1\n",
    "print \"Sentimiento negativo en los datos de prueba: %d\"%negativo_test\n",
    "print \"Sentimiento positivo en los datos de prueba: %d\"%positivo_test\n",
    "print train_df.shape\n",
    "print test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa en el resultado, existen una cantidad de 3554 registros de prueba al igual que de entrenamiento. Estas se dividen en 1803 que se clasifican como negativos y 1751 como positivos en ambos sets de datos.\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.b. Construcción y evaluación de una función con lower-casing y stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************Stemmer****************\n",
      " love eat cake\n",
      "I love to eat cake\n",
      "************************************\n",
      " love eat cake\n",
      "I love eating cake\n",
      "************************************\n",
      " love eat cake\n",
      "I loved eating the cake\n",
      "************************************\n",
      " love eat cake\n",
      "I do not love eating cake\n",
      "************************************\n",
      " n't love eat cake\n",
      "I don't love eating cake\n",
      "************************************\n",
      " hate thi work\n",
      "I hate this work\n",
      "************************************\n",
      " steam bad choic\n",
      "Steamming is a bad choice\n",
      "************************************\n",
      " lemmat bad choic\n",
      "lEMMATIZER IS a bad choice\n",
      "************************************\n",
      " thi work\n",
      "this not work\n",
      "************************************\n",
      " 'm work\n",
      "I'm WoRkInG\n",
      "************************************\n",
      " walk walker , , walk wall . . ...\n",
      "walking to the walker, , walked wall. . ...\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "#2.b\n",
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def word_extractor(text):\n",
    "        #wordlemmatizer = WordNetLemmatizer()\n",
    "        stemmer = PorterStemmer()\n",
    "        commonwords = stopwords.words('english')\n",
    "        text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "        words = \"\"\n",
    "        wordtokens = [ stemmer.stem(word.lower()) \\\n",
    "                                for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "        for word in wordtokens:\n",
    "                if word not in commonwords:\n",
    "                                words+=\" \"+word\n",
    "        return words\n",
    "\n",
    "print \"**************Stemmer****************\"\n",
    "print word_extractor(\"I love to eat cake\")\n",
    "print \"I love to eat cake\"\n",
    "print \"************************************\"\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print \"I love eating cake\"\n",
    "print \"************************************\"\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print \"I loved eating the cake\"\n",
    "print \"************************************\"\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print \"I do not love eating cake\"\n",
    "print \"************************************\"\n",
    "print word_extractor(\"I don't love eating cake\")\n",
    "print \"I don't love eating cake\"\n",
    "print \"************************************\"\n",
    "print word_extractor(\"I hate this work\")\n",
    "print \"I hate this work\"\n",
    "print \"************************************\"\n",
    "print word_extractor(\"Steamming is a bad choice\")\n",
    "print \"Steamming is a bad choice\"\n",
    "print \"************************************\"\n",
    "print word_extractor(\"lEMMATIZER IS a bad choice\")\n",
    "print \"lEMMATIZER IS a bad choice\"\n",
    "print \"************************************\"\n",
    "print word_extractor(\"this not work\") \n",
    "print \"this not work\"\n",
    "print \"************************************\"\n",
    "print word_extractor(\"I'm WoRkInG\") \n",
    "print \"I'm WoRkInG\"\n",
    "print \"************************************\"\n",
    "print word_extractor(\"walking to the walker, , walked wall. . ...\") \n",
    "print \"walking to the walker, , walked wall. . ...\"\n",
    "print \"************************************\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa en los ejemplos, Stemmer es capaz de detectar las palabras sin significado explícito (pronombres, artículos, preposiciones) y las elimina del texto, además, de traspasar todo el texto a minúsculas e identifica los signos de puntuación y comas (aunque estén separados o unidos a una palabra). Aun así, tiene problemas con textos en donde se abrevian palabras (como por ejemplo el don't y do not). También, es capaz de reducir algunas de las palabras y sus conjugaciones a su forma \"atómica\" para poder considerarlas dentro de un mismo grupo, como por ejemplo los verbos \"love\",\"walk\" y \"eat\".\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.c. Construcción y evaluacón de una función con lematice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Lemmatizer**********\n",
      " love eat cake\n",
      "I love to eat cake\n",
      "************************************\n",
      " love eating cake\n",
      "I love eating cake\n",
      "************************************\n",
      " loved eating cake\n",
      "I loved eating the cake\n",
      "************************************\n",
      " love eating cake\n",
      "I do not love eating cake\n",
      "************************************\n",
      " n't love eating cake\n",
      "I don't love eating cake\n",
      "************************************\n",
      " hate work\n",
      "I hate this work\n",
      "************************************\n",
      " steamming bad choice\n",
      "Steamming is a bad choice\n",
      "************************************\n",
      " lemmatizer bad choice\n",
      "lEMMATIZER IS a bad choice\n",
      "************************************\n",
      " work\n",
      "this not work\n",
      "************************************\n",
      " 'm working\n",
      "I'm WoRkInG\n",
      "************************************\n",
      " walking walker , , walked wall . . ...\n",
      "walking to the walker, , walked wall. . ...\n",
      "************************************\n",
      "PALBRA: 'This'\n",
      "LEMMATIZER: ''\n",
      "STEMMER: ' thi'\n",
      "PALBRA: 'Movie'\n",
      "LEMMATIZER: ' movie'\n",
      "STEMMER: ' movi'\n"
     ]
    }
   ],
   "source": [
    "# 2.c\n",
    "def word_extractor2(text):\n",
    "        wordlemmatizer = WordNetLemmatizer()\n",
    "        commonwords = stopwords.words('english')\n",
    "        text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "        words = \"\"\n",
    "        wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "                                for word in word_tokenize(text.decode('utf-8','ignore')) ]\n",
    "        for word in wordtokens:\n",
    "                if word not in commonwords:\n",
    "                                words+=\" \"+word\n",
    "        return words\n",
    "    \n",
    "print \"*********Lemmatizer**********\"\n",
    "print word_extractor2(\"I love to eat cake\")\n",
    "print \"I love to eat cake\"\n",
    "print \"************************************\"\n",
    "print word_extractor2(\"I love eating cake\")\n",
    "print \"I love eating cake\"\n",
    "print \"************************************\"\n",
    "print word_extractor2(\"I loved eating the cake\")\n",
    "print \"I loved eating the cake\"\n",
    "print \"************************************\"\n",
    "print word_extractor2(\"I do not love eating cake\")\n",
    "print \"I do not love eating cake\"\n",
    "print \"************************************\"\n",
    "print word_extractor2(\"I don't love eating cake\")\n",
    "print \"I don't love eating cake\"\n",
    "print \"************************************\"\n",
    "print word_extractor2(\"I hate this work\")\n",
    "print \"I hate this work\"\n",
    "print \"************************************\"\n",
    "print word_extractor2(\"Steamming is a bad choice\")\n",
    "print \"Steamming is a bad choice\"\n",
    "print \"************************************\"\n",
    "print word_extractor2(\"lEMMATIZER IS a bad choice\")\n",
    "print \"lEMMATIZER IS a bad choice\"\n",
    "print \"************************************\"\n",
    "print word_extractor2(\"this not work\") \n",
    "print \"this not work\"\n",
    "print \"************************************\"\n",
    "print word_extractor2(\"I'm WoRkInG\") \n",
    "print \"I'm WoRkInG\"\n",
    "print \"************************************\"\n",
    "print word_extractor2(\"walking to the walker, , walked wall. . ...\") \n",
    "print \"walking to the walker, , walked wall. . ...\"\n",
    "print \"************************************\"\n",
    "\n",
    "print \"PALBRA: 'This'\"\n",
    "print \"LEMMATIZER: '\" +word_extractor2(\"This\")+\"'\"\n",
    "print \"STEMMER: '\" +word_extractor(\"This\")+\"'\"\n",
    "\n",
    "\n",
    "print \"PALBRA: 'Movie'\"\n",
    "print \"LEMMATIZER: '\" +word_extractor2(\"Movie\")+\"'\"\n",
    "print \"STEMMER: '\" +word_extractor(\"Movie\")+\"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que lematizar las palabras produce un resultado similar a stemming + lower-casing, con las mismas dificultades en las palabras con abreviaciones, pero sin llevar las palabras a un mismo \"tronco lingüístico\" por lo que se contabilizan como palabras diferentes o sin relación todas las conjugaciones de, por ejemplo, la palabra \"love\"/\"loved\". Por último, una diferencia entre ambos en la reducción de pronombres, como por ejemplo, en la palabra \"This\" en donde lemmatizer la elimina totalmente, en cambio stemmer la mantiene como una base \"thi\".\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.d. Explorando el vocabulario y determinando las palabras más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************Stemmer****************\n",
      "\n",
      "--------------Train------------------\n",
      "\n",
      "[[u'film', 572], [u'ha', 238], [u'like', 248], [u'movi', 530], [u'one', 250], [u'thi', 514]]\n",
      "\n",
      "\n",
      "---------------Test-------------------\n",
      "\n",
      "[[u'film', 572], [u'ha', 238], [u'like', 248], [u'movi', 530], [u'one', 250], [u'thi', 514]]\n",
      "\n",
      "\n",
      "\n",
      "*********Lemmatizer**********\n",
      "\n",
      "-----------Train-------------\n",
      "\n",
      "[[u'film', 558], [u'ha', 238], [u'like', 230], [u'movie', 540], [u'one', 250]]\n",
      "\n",
      "------------Test--------------\n",
      "\n",
      "[[u'film', 558], [u'ha', 238], [u'like', 230], [u'movie', 540], [u'one', 250]]\n"
     ]
    }
   ],
   "source": [
    "#2.d\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "mini = 200\n",
    "#Stemmer\n",
    "print \"**************Stemmer****************\"\n",
    "print \"\"\n",
    "print \"--------------Train------------------\"\n",
    "print \"\"\n",
    "texts_train = [word_extractor(text) for text in train_df.Text]\n",
    "texts_test = [word_extractor(text) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0)\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "auxiliar = 0\n",
    "n_datos = 0\n",
    "maximo = 0\n",
    "for tag, count in zip(vocab, dist):\n",
    "    n_datos+=1\n",
    "    auxiliar+=count\n",
    "    if count>maximo:\n",
    "        maximo=count\n",
    "promedio=float(auxiliar)/float(n_datos)\n",
    "utilizadas_train =[]\n",
    "for tag, count in zip(vocab, dist):\n",
    "    if count>(promedio+mini):\n",
    "        utilizadas_train.append([tag,count])\n",
    "print utilizadas_train\n",
    "print \"\"\n",
    "print \"\"\n",
    "print \"---------------Test-------------------\"\n",
    "print \"\"\n",
    "auxiliar_test = 0\n",
    "n_datos_test = 0\n",
    "maximo_test = 0\n",
    "dist_test = list(np.array(features_test.sum(axis=0)).reshape(-1,))\n",
    "for tag, count in zip(vocab, dist_test):\n",
    "    n_datos_test+=1\n",
    "    auxiliar_test+=count\n",
    "    if count>maximo_test:\n",
    "        maximo_test=count\n",
    "promedio_test=float(auxiliar_test)/float(n_datos_test)\n",
    "utilizadas_test =[]\n",
    "for tag, count in zip(vocab, dist_test):\n",
    "    if count>(promedio_test+mini):\n",
    "        utilizadas_test.append([tag,count])\n",
    "print utilizadas_test\n",
    "print \"\"\n",
    "print \"\"\n",
    "print \"\"\n",
    "#Lemamtizer\n",
    "print \"*********Lemmatizer**********\"\n",
    "print \"\"\n",
    "print \"-----------Train-------------\"\n",
    "print \"\"\n",
    "texts_train_lem = [word_extractor2(text) for text in train_df.Text]\n",
    "texts_test_lem = [word_extractor2(text) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "features_train_lem = vectorizer.transform(texts_train_lem)\n",
    "features_test_lem = vectorizer.transform(texts_test_lem)\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0)\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train_lem.sum(axis=0)).reshape(-1,))\n",
    "auxiliar = 0\n",
    "n_datos = 0\n",
    "maximo = 0\n",
    "for tag, count in zip(vocab, dist):\n",
    "    n_datos+=1\n",
    "    auxiliar+=count\n",
    "    if count>maximo:\n",
    "        maximo=count\n",
    "promedio=float(auxiliar)/float(n_datos)\n",
    "utilizadas_train_lem =[]\n",
    "for tag, count in zip(vocab, dist):\n",
    "    if count>(promedio+mini):\n",
    "        utilizadas_train_lem.append([tag,count])\n",
    "print utilizadas_train_lem\n",
    "print \"\"\n",
    "print \"------------Test--------------\"\n",
    "print \"\"\n",
    "auxiliar_test = 0\n",
    "n_datos_test = 0\n",
    "maximo_test = 0\n",
    "dist_test = list(np.array(features_test_lem.sum(axis=0)).reshape(-1,))\n",
    "for tag, count in zip(vocab, dist_test):\n",
    "    n_datos_test+=1\n",
    "    auxiliar_test+=count\n",
    "    if count>maximo_test:\n",
    "        maximo_test=count\n",
    "promedio_test=float(auxiliar_test)/float(n_datos_test)\n",
    "utilizadas_test_lem =[]\n",
    "for tag, count in zip(vocab, dist_test):\n",
    "    if count>(promedio_test+mini):\n",
    "        utilizadas_test_lem.append([tag,count])\n",
    "print utilizadas_test_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según lo obtenido por Stemmer, las palabras más frecuentes en ambos sets son \"film\" 572 veces, \"movie\" 530 veces, \"one\" 250 veces, \"like\" 248 veces, \"ha\"(que probablemente sea la el tronco lingüístico del verbo \"has\"(tiene)) 238 veces  y \"thi\"(que probablemente sea la abreviación de \"this\") con 514 veces.\n",
    "\n",
    "Por otro lado, Lemmatizer obtiene con mayor frecuencia para ambos sets de datos las palabras \"film\"(558) , \"ha\"(238), \"like\"(230), \"movie\"(540), \"one\"(250).\n",
    "\n",
    "Al comparar ambos resultados se puede dar cuenta de que la mayoría de las palabras se repiten, algunas contabilizadas la misma cantidad de veces y otras como \"thi\" eliminada en Lemmatizer por no tener relevancia para el análisis. \n",
    "\n",
    "Cabe agregar que Stemmer empeora algunas palabras (como \"movie\"->\"movi\") lo que puede empeorar la contabilización. Esto se puede deber a que considera que algunas palabras son plurales de forma erronea, generando equivocaciones en los conteos y creando nuevos \"troncos lingüísticos\" inexistentes.\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.e. Explicación de las métricas que calcula la función classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.e\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "accuracy_dict_train={}\n",
    "accuracy_dict_test={}\n",
    "contador_universal_svm=0\n",
    "contador_universal_logistic=0\n",
    "\n",
    "def score_the_model(model,x,y,xt,yt,text):\n",
    "    global accuracy_dict_test\n",
    "    global accuracy_dict_train\n",
    "    global contador_universal_logistic\n",
    "    global contador_universal_svm\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    if text!=\"LOGISTIC\" and text!=\"SVM\":\n",
    "        accuracy_dict_train[text+\"_train\"]=acc_tr\n",
    "        accuracy_dict_test[text+\"_test\"]=acc_test\n",
    "    if text==\"LOGISTIC\":\n",
    "        contador_universal_logistic+=1\n",
    "        accuracy_dict_train[text+\"_train_\"+str(contador_universal_logistic)]=acc_tr\n",
    "        accuracy_dict_test[text+\"_test_\"+str(contador_universal_logistic)]=acc_test\n",
    "    if text==\"SVM\":\n",
    "        contador_universal_svm+=1\n",
    "        accuracy_dict_train[text+\"_train_\"+str(contador_universal_svm)]=acc_tr\n",
    "        accuracy_dict_test[text+\"_test_\"+str(contador_universal_svm)]=acc_test\n",
    "    print \"Training Accuracy %s: %f\"%(text,acc_tr)\n",
    "    print \"Test Accuracy %s: %f\"%(text,acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función classification_report entrega 4 métricas que corresponden a precision, recall, f1-score y support. La métrica “precision” corresponde a la fracción de casos recuperados que son relevantes, mientras que la métrica “recall” (también conocido como sensibilidad) es la fracción de los casos pertinentes que se recuperan. Estas 2 métricas se basan en una comprensión y medida de relevancia. Por otro lado la métrica “f1-score” es una medida de la precisión en una prueba, este puntaje se calcula como $F1-score = \\frac{2\\cdot(precision\\cdot recall)}{precision+recall}$. F1-score se puede interpretar como un promedio ponderado de la precision y el recall. Por último la métrica “support” indica cuantos valores corresponden a una cierta clase.\n",
    "\n",
    "\n",
    "### 2.2.f. Predicción de sentimientos mediante un clasificador Bayesiano Ingenuo (Binario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer:\n",
      " \n",
      "Training Accuracy BernoulliNB: 0.932752\n",
      "Test Accuracy BernoulliNB: 0.932733\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.92      0.95      0.93      1803\n",
      "          -       0.94      0.92      0.93      1751\n",
      "\n",
      "avg / total       0.93      0.93      0.93      3554\n",
      "\n",
      "[ 0.97121959  0.02878041] it's all pretty tame . the most offensive thing about the movie is that hollywood expects people to pay to see it .\n",
      "\n",
      "[ 0.6456771  0.3543229] it is , by conventional standards , a fairly terrible movie . . . but it is also weirdly fascinating , a ready-made eurotrash cult object . it is also , at times , curiously moving .\n",
      "\n",
      "[ 0.92221791  0.07778209] . . . while certainly clever in spots , this too-long , spoofy update of shakespeare's macbeth doesn't sustain a high enough level of invention .\n",
      "\n",
      "[ 0.94946154  0.05053846] there's only one way to kill michael myers for good : stop buying tickets to these movies .\n",
      "\n",
      "[ 0.01055554  0.98944446] the last kiss will probably never achieve the popularity of my big fat greek wedding , but its provocative central wedding sequence has far more impact .\n",
      "\n",
      "[ 0.00119938  0.99880062] still rapturous after all these years , cinema paradiso stands as one of the great films about movie love .\n",
      "\n",
      "[ 0.16538873  0.83461127] the performances are an absolute joy .\n",
      "\n",
      "[ 0.98666656  0.01333344] even when crush departs from the 4w formula . . . it feels like a glossy rehash .\n",
      "\n",
      "[ 0.13658854  0.86341146] a perceptive , good-natured movie .\n",
      "\n",
      "[ 0.36025821  0.63974179] de niro and mcdormand give solid performances , but their screen time is sabotaged by the story's inability to create interest .\n",
      "\n",
      "[ 0.01108154  0.98891846] as any creature-feature fan knows , when you cross toxic chemicals with a bunch of exotic creatures , you get a lot of running around , screaming and death . on that score , the film certainly doesn't disappoint .\n",
      "\n",
      "[ 0.02085574  0.97914426] in his u . s . debut , mr . schnitzler proves himself a deft pace master and stylist .\n",
      "\n",
      "[ 0.36871262  0.63128738] a cautionary tale about the grandiosity of a college student who sees himself as impervious to a fall .\n",
      "\n",
      "[ 0.84530644  0.15469356] a full world has been presented onscreen , not some series of carefully structured plot points building to a pat resolution .\n",
      "\n",
      "[  4.71878046e-04   9.99528122e-01] a heady , biting , be-bop ride through nighttime manhattan , a loquacious videologue of the modern male and the lengths to which he'll go to weave a protective cocoon around his own ego .\n",
      "\n",
      "--------------------------------------------\n",
      "Lemmatizer:\n",
      " \n",
      "Training Accuracy BernoulliNB: 0.875914\n",
      "Test Accuracy BernoulliNB: 0.875880\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.86      0.90      0.88      1803\n",
      "          -       0.89      0.85      0.87      1751\n",
      "\n",
      "avg / total       0.88      0.88      0.88      3554\n",
      "\n",
      "[ 0.80849996  0.19150004] little more than a stylish exercise in revisionism whose point . . . is no doubt true , but serves as a rather thin moral to such a knowing fable .\n",
      "\n",
      "[ 0.30470395  0.69529605] the story has some nice twists but the ending and some of the back-story is a little tired . the performances are all solid ; it merely lacks originality to make it a great movie .\n",
      "\n",
      "[ 0.81654479  0.18345521] the screenplay by james eric , james horton and director peter o'fallon . . . is so pat it makes your teeth hurt .\n",
      "\n",
      "[ 0.04665703  0.95334297] this is dicaprio's best performance in anything ever , and easily the most watchable film of the year .\n",
      "\n",
      "[ 0.89519903  0.10480097] barry convinces us he's a dangerous , secretly unhinged guy who could easily have killed a president because it made him feel powerful .\n",
      "\n",
      "[ 0.71868721  0.28131279] hey , at least the title of this film lets you know exactly where it's heading .\n",
      "\n",
      "[ 0.72818833  0.27181167] a delicious , quirky movie with a terrific screenplay and fanciful direction by michael gondry .\n",
      "\n",
      "[ 0.28745968  0.71254032] a very good film sits in the place where a masterpiece should be .\n",
      "\n",
      "[ 0.06782643  0.93217357] for all its brooding quality , ash wednesday is suspenseful and ultimately unpredictable , with a sterling ensemble cast .\n",
      "\n",
      "[ 0.00777643  0.99222357] whatever complaints i might have , i'd take [its] earnest errors and hard-won rewards over the bombastic self-glorification of other feel-good fiascos like antwone fisher or the emperor's club any time .\n",
      "\n",
      "[ 0.93202848  0.06797152] a culture clash comedy only half as clever as it thinks it is .\n",
      "\n",
      "[ 0.3647528  0.6352472] the film is bright and flashy in all the right ways .\n",
      "\n",
      "[  9.92255199e-04   9.99007745e-01] with an expressive face reminiscent of gong li and a vivid personality like zhang ziyi's , dong stakes out the emotional heart of happy .\n",
      "\n",
      "[ 0.00237029  0.99762971] without resorting to hyperbole , i can state that kissing jessica stein may be the best same-sex romance i have seen .\n",
      "\n",
      "[ 0.08839408  0.91160592] a film that's flawed and brilliant in equal measure .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.f\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import random\n",
    "def do_NAIVE_BAYES(x,y,xt,yt):\n",
    "    model = BernoulliNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "    return model\n",
    "\n",
    "#Stemmer\n",
    "\n",
    "print \"Stemmer:\"\n",
    "print \" \"\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text\n",
    "    \n",
    "#Lemmatizer\n",
    "\n",
    "print \"--------------------------------------------\"\n",
    "print \"Lemmatizer:\"\n",
    "print \" \"\n",
    "model_lem=do_NAIVE_BAYES(features_train_lem,labels_train,features_test_lem,labels_test)\n",
    "test_pred_lem = model_lem.predict_proba(features_test_lem)\n",
    "spl = random.sample(xrange(len(test_pred_lem)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred_lem[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado los resultados, utilizando el clasificador Bayesiano Ingenuo en Stemmer, se obtiene un Accuracy para los datos de entrenamiento de 0.932752 y un Accuracy para los datos de prueba de 0.932733, la diferencia en rendimiento entre ambos datos es casi insignificante. En el caso de Lemmatizer, se obtiene un Accuracy para los datos de entrenamiento de 0.875914 y un Accuracy para los datos de prueba de 0.875880, al igual que antes, la diferencia tambien es casi insignificante, ahora, comparando Stemmer vs Lemmatizer, se observa que Lemmatizer obtiene un rendimiento superior, pero solo por una diferencia cercana al 6% tanto en los datos de entrenamiento como los de prueba. \n",
    "\n",
    "Respecto a las metricas dadas por classification_report, tanto la precision, el recall y el f1-score para las 2 clases de sentimientos, se obtiene un valor muy cercano al accuracy obtenido anteriormiente, tanto para los datos de entrenamiento como los de prueba. Por otro lado, se vuelve a observar que Lemmatizer obtiene un rendimiento superior.\n",
    "\n",
    "Finalmente se obtiene un valor entre 0 y 1 tanto para el sentimiento positivo como negativo junto con la frase donde fue clasificado el sentimiento de la forma [negativo positivo]. Como se puede notar tanto para Stemmer como Lemmatizer, se clasifica de buena manera el grado del sentimiento para cada comentario. \n",
    "\n",
    "### 2.2.g. Predicción de sentimientos mediante un clasificador Bayesiano Multinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer\n",
      " \n",
      "Training Accuracy MULTINOMIAL: 0.933596\n",
      "Test Accuracy MULTINOMIAL: 0.933577\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.93      0.94      0.93      1803\n",
      "          -       0.94      0.93      0.93      1751\n",
      "\n",
      "avg / total       0.93      0.93      0.93      3554\n",
      "\n",
      "[ 0.99231763  0.00768237] the movie would seem less of a trifle if ms . sugarman followed through on her defiance of the saccharine .\n",
      "\n",
      "[ 0.98766417  0.01233583] so faithful to the doldrums of the not-quite-urban , not-quite-suburban milieu as to have viewers recoiling from the reality check .\n",
      "\n",
      "[ 0.01855483  0.98144517] a vibrant , colorful , semimusical rendition .\n",
      "\n",
      "[ 0.14594286  0.85405714] feral and uncomfortable .\n",
      "\n",
      "[ 0.26671077  0.73328923] the movie should be credited with remembering his victims .\n",
      "\n",
      "[ 0.03165645  0.96834355] the chateau is a risky venture that never quite goes where you expect and often surprises you with unexpected comedy .\n",
      "\n",
      "[ 0.16558432  0.83441568] [an] absorbing documentary .\n",
      "\n",
      "[ 0.05943477  0.94056523] one of the most significant moviegoing pleasures of the year .\n",
      "\n",
      "[ 0.00511454  0.99488546] one of the best examples of how to treat a subject , you're not fully aware is being examined , much like a photo of yourself you didn't know was being taken .\n",
      "\n",
      "[  6.64455261e-04   9.99335545e-01] director andrew niccol . . . demonstrates a wry understanding of the quirks of fame . his healthy sense of satire is light and fun . . . .\n",
      "\n",
      "[ 0.23663598  0.76336402] the entire cast is extraordinarily good .\n",
      "\n",
      "[ 0.43512922  0.56487078] go for la salle's performance , and make do as best you can with a stuttering script .\n",
      "\n",
      "[ 0.43741603  0.56258397] less than fresh .\n",
      "\n",
      "[ 0.02230291  0.97769709] nothing short of wonderful with its ten-year-old female protagonist and its steadfast refusal to set up a dualistic battle between good and evil .\n",
      "\n",
      "[ 0.00891791  0.99108209] it gives devastating testimony to both people's capacity for evil and their heroic capacity for good .\n",
      "\n",
      "--------------------------------------------\n",
      "Lemmatizer\n",
      " \n",
      "Training Accuracy MULTINOMIAL: 0.877040\n",
      "Test Accuracy MULTINOMIAL: 0.877005\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.87      0.89      0.88      1803\n",
      "          -       0.89      0.86      0.87      1751\n",
      "\n",
      "avg / total       0.88      0.88      0.88      3554\n",
      "\n",
      "[ 0.51590719  0.48409281] these characters become wearisome .\n",
      "\n",
      "[ 0.97490407  0.02509593] it stars schticky chris rock and stolid anthony hopkins , who seem barely in the same movie . their contrast is neither dramatic nor comic -- it's just a weird fizzle .\n",
      "\n",
      "[ 0.13103859  0.86896141] like the original , this version is raised a few notches above kiddie fantasy pablum by allen's astringent wit .\n",
      "\n",
      "[ 0.01635855  0.98364145] a quietly reflective and melancholy new zealand film about an eventful summer in a 13-year-old girl's life .\n",
      "\n",
      "[ 0.53073095  0.46926905] a tv episode inflated past its natural length .\n",
      "\n",
      "[ 0.97167926  0.02832074] nothing more than four or five mild chuckles surrounded by 86 minutes of overly-familiar and poorly-constructed comedy .\n",
      "\n",
      "[ 0.00268517  0.99731483] despite a quieter middle section , involving aragorn's dreams of arwen , this is even better than the fellowship . there are scenes of cinematic perfection that steal your heart away .\n",
      "\n",
      "[ 0.03296799  0.96703201] richard gere and diane lane put in fine performances as does french actor oliver martinez .\n",
      "\n",
      "[ 0.15434646  0.84565354] this is such a dazzlingly self-assured directorial debut that it's hard to know what to praise first .\n",
      "\n",
      "[ 0.03743894  0.96256106] well-done supernatural thriller with keen insights into parapsychological phenomena and the soulful nuances of the grieving process .\n",
      "\n",
      "[ 0.98846937  0.01153063] one of the most depressing movie-going experiences i can think of is to sit through about 90 minutes of a so-called 'comedy' and not laugh once .\n",
      "\n",
      "[ 0.99860968  0.00139032] our culture is headed down the toilet with the ferocity of a frozen burrito after an all-night tequila bender � and i know this because i've seen 'jackass : the movie . '\n",
      "\n",
      "[ 0.4244546  0.5755454] everything in maid in manhattan is exceedingly pleasant , designed not to offend . it goes down easy , leaving virtually no aftertaste .\n",
      "\n",
      "[ 0.26443065  0.73556935] its spirit of iconoclastic abandon -- however canned -- makes for unexpectedly giddy viewing .\n",
      "\n",
      "[ 0.00646336  0.99353664] <em>ash wednesday</em> is not edward burns' best film , but it is a good and ambitious film . and it marks him as one of the most interesting writer/directors working today .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.g\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import random\n",
    "def do_MULTINOMIAL(x,y,xt,yt):\n",
    "    model = MultinomialNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"MULTINOMIAL\")\n",
    "    return model\n",
    "\n",
    "#Stemmer\n",
    "\n",
    "print \"Stemmer\"\n",
    "print \" \"\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text\n",
    "\n",
    "#Lemmatizer\n",
    "print \"--------------------------------------------\"\n",
    "print \"Lemmatizer\"\n",
    "print \" \"\n",
    "model_lem=do_MULTINOMIAL(features_train_lem,labels_train,features_test_lem,labels_test)\n",
    "test_pred_lem = model_lem.predict_proba(features_test_lem)\n",
    "spl = random.sample(xrange(len(test_pred_lem)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred_lem[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado los resultados, utilizando el clasificador Bayesiano Ingenuo Multinomial en Stemmer, se obtiene un Accuracy para los datos de entrenamiento de 0.933596 y un Accuracy para los datos de prueba de 0.933577, la diferencia en rendimiento entre ambos datos es casi insignificante. En el caso de Lemmatizer, se obtiene un Accuracy para los datos de entrenamiento de 0.877040 y un Accuracy para los datos de prueba de 0.877005, al igual que antes, la diferencia tambien es casi despreciable, ahora, comparando Stemmer vs Lemmatizer, se observa que el segundo obtiene un rendimiento superior, pero solo por una diferencia de cercana al 6% tanto en los datos de entrenamiento como los de prueba. Cabe destacar que el rendimiento mostrado por este clasificador es bastante parejo al clasificador utilizado en la pregunta anterior (Bayesiano Ingenuo Binario). \n",
    "\n",
    "Respecto a las metricas dadas por classification_report, tanto la precision, el recall y el f1-score para las 2 clases de sentimientos, se obtiene un valor muy cercano al accuracy obtenido anteriormiente, tanto para los datos de entrenamiento como los de prueba. También se vuelve a observar que Lemmatizer obtiene un rendimiento superior.\n",
    "\n",
    "Finalmente se obtiene un valor entre 0 y 1 tanto para el sentimiento positivo como negativo junto con la frase donde fue clasificado el sentimiento de la forma [negativo positivo]. Como se puede notar tanto para Stemmer como Lemmatizer, se clasifica de buena manera el grado del sentimiento para cada comentario. Comparando respecto al clasificador utilizado en la pregunta anterior, existe en algunos casos, una mayor diferencia en los valores de los sentimientos negativo y positivo. \n",
    "\n",
    "\n",
    "### 2.2.h1. Predicción de sentimientos mediante un modelo de Regresión Logística Regularizado (Penalizando con la norma $l_{2}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer\n",
      " \n",
      "Usando C = 0.010000\n",
      "Training Accuracy LOGISTIC: 0.787001\n",
      "Test Accuracy LOGISTIC: 0.786941\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.77      0.84      0.80      1803\n",
      "          -       0.81      0.74      0.77      1751\n",
      "\n",
      "avg / total       0.79      0.79      0.79      3554\n",
      "\n",
      "Usando C = 0.100000\n",
      "Training Accuracy LOGISTIC: 0.892234\n",
      "Test Accuracy LOGISTIC: 0.892204\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.88      0.91      0.90      1803\n",
      "          -       0.90      0.87      0.89      1751\n",
      "\n",
      "avg / total       0.89      0.89      0.89      3554\n",
      "\n",
      "Usando C = 10.000000\n",
      "Training Accuracy LOGISTIC: 0.999156\n",
      "Test Accuracy LOGISTIC: 0.999156\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       1.00      1.00      1.00      1803\n",
      "          -       1.00      1.00      1.00      1751\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3554\n",
      "\n",
      "Usando C = 100.000000\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 1.000000\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       1.00      1.00      1.00      1803\n",
      "          -       1.00      1.00      1.00      1751\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3554\n",
      "\n",
      "Usando C = 1000.000000\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 1.000000\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       1.00      1.00      1.00      1803\n",
      "          -       1.00      1.00      1.00      1751\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3554\n",
      "\n",
      "----------\n",
      "Lemmatizer\n",
      " \n",
      "Usando C = 0.010000\n",
      "Training Accuracy LOGISTIC: 0.719471\n",
      "Test Accuracy LOGISTIC: 0.719392\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.70      0.78      0.74      1803\n",
      "          -       0.75      0.65      0.70      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Usando C = 0.100000\n",
      "Training Accuracy LOGISTIC: 0.805571\n",
      "Test Accuracy LOGISTIC: 0.805516\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.80      0.83      0.81      1803\n",
      "          -       0.81      0.78      0.80      1751\n",
      "\n",
      "avg / total       0.81      0.81      0.81      3554\n",
      "\n",
      "Usando C = 10.000000\n",
      "Training Accuracy LOGISTIC: 0.967361\n",
      "Test Accuracy LOGISTIC: 0.967352\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.96      0.97      0.97      1803\n",
      "          -       0.97      0.96      0.97      1751\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3554\n",
      "\n",
      "Usando C = 100.000000\n",
      "Training Accuracy LOGISTIC: 0.984243\n",
      "Test Accuracy LOGISTIC: 0.984239\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.98      0.99      0.98      1803\n",
      "          -       0.99      0.98      0.98      1751\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3554\n",
      "\n",
      "Usando C = 1000.000000\n",
      "Training Accuracy LOGISTIC: 0.990152\n",
      "Test Accuracy LOGISTIC: 0.990149\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.99      1.00      0.99      1803\n",
      "          -       0.99      0.99      0.99      1751\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.h1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def do_LOGIT(x,y,xt,yt):\n",
    "    start_t = time.time()\n",
    "    Cs = [0.01,0.1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"Usando C = %f\"%C\n",
    "        model = LogisticRegression(penalty='l2',C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "        \n",
    "#Stemmer\n",
    "\n",
    "print \"Stemmer\"\n",
    "print \" \"\n",
    "do_LOGIT(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "#Lemmatizer\n",
    "\n",
    "print \"----------\"\n",
    "print \"Lemmatizer\"\n",
    "print \" \"\n",
    "do_LOGIT(features_train_lem,labels_train,features_test_lem,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar los resultados utilizando un modelo de Regresión Logística Regularizado con una penalización de [0.01, 0.1, 10, 100, 1000], se puede ver con Stemmer que al penalizar demasiado el modelo, tanto el Accuracy de entrenamiento como el de prueba van empeorando por castigar demasiado. Entonces no es recomendado utilizar un parametro de penalización grande. Estos resultados quedan demostrados nuevamente con las 4 metricas.\n",
    "\n",
    "Observando los resultados con Lemmatizer, se puede notar un comportamiento similar que en Stemmer. Al aumentar el parametro de penalizacion, el error aumenta, pero este varia mas lentamente que Stemmer, por lo que se puede ver que el castigo realizado es mas leve. Cabe destacar que al comparar el parametro de penalización 100 y 1000, este ultimo tuvo un rendimiento levemente mejor.\n",
    "\n",
    "Al comparar Stemmer vs Lemmatizer, con los parametros [0.01, 0.1, 10, 100, 1000], Lemmatizer obtiene un mejor rendimiento que Stemmer. Con parametro 0.01 la diferencia de rendimiento es 9% aproximadamente. Con el resto de parametros esta diferencia va disminuyendo, pero aun asi, Lemmatizer tiene siempe un rendimiento levemente mejor que Stemmer.\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.h2. Predicción de sentimientos mediante una Máquina de Vectores de Soporte (SVM) Lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer\n",
      "El valor de C que se esta probando: 0.010000\n",
      "Training Accuracy SVM: 0.886607\n",
      "Test Accuracy SVM: 0.886575\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.87      0.91      0.89      1803\n",
      "          -       0.90      0.87      0.88      1751\n",
      "\n",
      "avg / total       0.89      0.89      0.89      3554\n",
      "\n",
      "El valor de C que se esta probando: 0.100000\n",
      "Training Accuracy SVM: 0.980023\n",
      "Test Accuracy SVM: 0.980017\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.97      0.99      0.98      1803\n",
      "          -       0.99      0.97      0.98      1751\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3554\n",
      "\n",
      "El valor de C que se esta probando: 10.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 1.000000\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       1.00      1.00      1.00      1803\n",
      "          -       1.00      1.00      1.00      1751\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3554\n",
      "\n",
      "El valor de C que se esta probando: 100.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 1.000000\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       1.00      1.00      1.00      1803\n",
      "          -       1.00      1.00      1.00      1751\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3554\n",
      "\n",
      "El valor de C que se esta probando: 1000.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 1.000000\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       1.00      1.00      1.00      1803\n",
      "          -       1.00      1.00      1.00      1751\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3554\n",
      "\n",
      "----------\n",
      "Lemmatizer\n",
      "El valor de C que se esta probando: 0.010000\n",
      "Training Accuracy SVM: 0.798818\n",
      "Test Accuracy SVM: 0.798762\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.79      0.82      0.81      1803\n",
      "          -       0.81      0.78      0.79      1751\n",
      "\n",
      "avg / total       0.80      0.80      0.80      3554\n",
      "\n",
      "El valor de C que se esta probando: 0.100000\n",
      "Training Accuracy SVM: 0.906866\n",
      "Test Accuracy SVM: 0.906839\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.90      0.92      0.91      1803\n",
      "          -       0.92      0.89      0.90      1751\n",
      "\n",
      "avg / total       0.91      0.91      0.91      3554\n",
      "\n",
      "El valor de C que se esta probando: 10.000000\n",
      "Training Accuracy SVM: 0.984524\n",
      "Test Accuracy SVM: 0.984520\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.98      0.99      0.98      1803\n",
      "          -       0.99      0.98      0.98      1751\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3554\n",
      "\n",
      "El valor de C que se esta probando: 100.000000\n",
      "Training Accuracy SVM: 0.985087\n",
      "Test Accuracy SVM: 0.985083\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.98      0.99      0.99      1803\n",
      "          -       0.99      0.98      0.98      1751\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3554\n",
      "\n",
      "El valor de C que se esta probando: 1000.000000\n",
      "Training Accuracy SVM: 0.976927\n",
      "Test Accuracy SVM: 0.976921\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.97      0.99      0.98      1803\n",
      "          -       0.98      0.97      0.98      1751\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.h2\n",
    "from sklearn.svm import LinearSVC\n",
    "def do_SVM(x,y,xt,yt):\n",
    "    Cs = [0.01,0.1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"El valor de C que se esta probando: %f\"%C\n",
    "        model = LinearSVC(C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"SVM\")\n",
    "        \n",
    "#Stemmer\n",
    "\n",
    "print \"Stemmer\"\n",
    "do_SVM(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "#Lematizer\n",
    "\n",
    "print \"----------\"\n",
    "print \"Lemmatizer\"\n",
    "do_SVM(features_train_lem,labels_train,features_test_lem,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar los resultados obtenidos con una Máquina de Vectores de Soporte Lineal con parametros de regularización [0.01, 0.1, 10, 100, 1000] se puede observar resultados similares a los obtenidos en la pregunta anterior, pero estos son mas drásticos, de hecho ya con Stemmer desde el parametro 0.1 en adelante el accuracy ya es muy alto (casi 1) para los datos de entrenamiento como los de prueba. Con Lemmatizer tambien ocurre resultados similares a la pregunta anterior (Regresión Logística Regularizada).\n",
    "\n",
    "Al comparar Stemmer vs Lemmatizer, con los parametros [0.01, 0.1, 10, 100, 1000], Lemmatizer obtiene un mejor rendimiento que Stemmer. Con parametro 0.01 la diferencia de rendimiento es de 8%-9% aproximadamente. Con el resto de parametros esta diferencia va disminuyendo, pero aun asi, Lemmatizer tiene un rendimiento levemente mejor que Stemmer.\n",
    "\n",
    "\n",
    "### 2.2.i. Gráfico de comparación resultados entre distintos metodos de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "LOGISTIC_test_3 |     0.999156     |     0.999156\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "LOGISTIC_test_2 |     0.892204     |     0.892234\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "LOGISTIC_test_1 |     0.786941     |     0.787001\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "LOGISTIC_test_7 |     0.805516     |     0.805571\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "LOGISTIC_test_6 |     0.719392     |     0.719471\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "LOGISTIC_test_5 |     1.000000     |     1.000000\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "LOGISTIC_test_4 |     1.000000     |     1.000000\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "LOGISTIC_test_9 |     0.984239     |     0.984243\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "LOGISTIC_test_8 |     0.967352     |     0.967361\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "MULTINOMIAL_test |     0.877005     |     0.877040\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "BernoulliNB_test |     0.875880     |     0.875914\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "SVM_test_10 |     0.976921     |     0.976927\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "SVM_test_8 |     0.984520     |     0.984524\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "SVM_test_9 |     0.985083     |     0.985087\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "LOGISTIC_test_10 |     0.990149     |     0.990152\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "SVM_test_4 |     1.000000     |     1.000000\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "SVM_test_5 |     1.000000     |     1.000000\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "SVM_test_6 |     0.798762     |     0.798818\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "SVM_test_7 |     0.906839     |     0.906866\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "SVM_test_1 |     0.886575     |     0.886607\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "SVM_test_2 |     0.980017     |     0.980023\n",
      " \n",
      "Clasificador | Accuracy_test | Accuracy_train\n",
      "SVM_test_3 |     1.000000     |     1.000000\n",
      " \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAMpCAYAAABfVzFmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ba9950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2.i\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "label_test = []\n",
    "label_train =[]\n",
    "data_test = []\n",
    "data_train = []\n",
    "for x in accuracy_dict_test:\n",
    "    print \"Clasificador | Accuracy_test | Accuracy_train\"\n",
    "    aux=x.split(\"_\",2)\n",
    "    if aux[0]=='LOGISTIC' or aux[0]=='SVM':\n",
    "        print \"%s |     %f     |     %f\"%(x, accuracy_dict_test[x],accuracy_dict_train[aux[0]+\"_train_\"+aux[2]])\n",
    "        print \" \"\n",
    "        label_test.append(x)\n",
    "        label_train.append(aux[0]+aux[2])\n",
    "        data_train.append(accuracy_dict_train[aux[0]+\"_train_\"+aux[2]])\n",
    "        data_test.append(accuracy_dict_test[x])\n",
    "    if aux[0]=='BernoulliNB' or aux[0]=='MULTINOMIAL' :\n",
    "        print \"%s |     %f     |     %f\"%(x, accuracy_dict_test[x],accuracy_dict_train[aux[0]+\"_train\"])\n",
    "        print \" \"\n",
    "        label_test.append(x)\n",
    "        label_train.append(aux[0])\n",
    "        data_train.append(accuracy_dict_train[aux[0]+\"_train\"])\n",
    "        data_test.append(accuracy_dict_test[x])\n",
    "auxiliar=[]\n",
    "\n",
    "for i in range(1,len(data_test)+1):\n",
    "    auxiliar.append(i)\n",
    "    \n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(auxiliar, data_test,'o',linestyle=' ', color='b')\n",
    "plt.plot(auxiliar,data_train , marker='*',linestyle='', color= 'r')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Clasificador')\n",
    "plt.xticks(auxiliar, label_train, rotation=90)\n",
    "plt.legend(['Train data error', 'Test data error'], loc='lower right')\n",
    "plt.gcf().subplots_adjust(bottom=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Luego de construir un gráfico con los 4 métodos de clasificación utilizados en las preguntas anteriores, se puede observar que el error de entrenamiento y de prueba son casi el mismo para cada metodo. Por otro lado, el metodo que obtiene mejor rendimiento a nivel global es Regresión Logistica Regularizada, el cual obtiene un Accurracy de 0.719392 para los datos de prueba y 0.719471 para los datos entrenamiento en el mejor caso. La variacion de rendimiento depende de que parametro de regularizacion se este usando para los métodos de SVM y Regresión Logistica Regularizada. Pero como se analizo en preguntas anteriores, con un parametro de poca magnitud (Por ende, poco castigo) el rendimiento es mejor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
